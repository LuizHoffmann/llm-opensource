import os
import sys
import logging
import streamlit as st
from dotenv import load_dotenv
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings
# CORRE√á√ÉO: A importa√ß√£o correta do Ollama √© feita a partir do langchain_community
from langchain_community.llms import Ollama
from langchain_qdrant import Qdrant
from qdrant_client import QdrantClient

# Importa e executa a configura√ß√£o do logger
from logger_config import setup_logger
setup_logger()
logger = logging.getLogger(__name__)

# --- Carregamento de Configura√ß√µes e Modelos (com Cache) ---
load_dotenv()

@st.cache_resource
def get_qdrant_client():
    """ Carrega o cliente Qdrant uma vez e o mant√©m em cache. """
    url = os.getenv("QDRANT_URL")
    api_key = os.getenv("QDRANT_API_KEY")
    if not url or not api_key:
        st.error("As vari√°veis de ambiente do Qdrant n√£o foram definidas. Verifique seu arquivo .env.")
        st.stop()
    return QdrantClient(url=url, api_key=api_key)

@st.cache_resource
def get_embeddings_model():
    """ Carrega o modelo de embeddings uma vez e o mant√©m em cache. """
    logger.info("Carregando modelo de embeddings...")
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    logger.info("Modelo de embeddings carregado.")
    return embeddings

# --- Configura√ß√£o do Pipeline RAG ---

def setup_qa_chain(collection_name):
    """ Configura e retorna a cadeia de RetrievalQA para uma cole√ß√£o espec√≠fica. """
    logger.info(f"Configurando a cadeia RAG para a cole√ß√£o: {collection_name}")
    client = get_qdrant_client()
    embeddings = get_embeddings_model()
    
    qdrant_vector_store = Qdrant(
        client=client, 
        collection_name=collection_name, 
        embeddings=embeddings
    )
    
    retriever = qdrant_vector_store.as_retriever(search_kwargs={"k": 5})
    llm = Ollama(model="llama3")
    
    prompt_template = """
    Voc√™ √© um assistente especializado nas normas e pol√≠ticas do Programa de Engenharia de Sistemas e Computa√ß√£o (PESC) da Coppe/UFRJ.
    Sua principal fun√ß√£o √© responder perguntas baseando-se estritamente nos documentos internos do programa.
    **Contexto Fornecido:**
    {context}
    **Instru√ß√µes:**
    1. Analise o contexto acima para formular sua resposta.
    2. Se a resposta n√£o puder ser encontrada no contexto, responda exatamente: 'Com base nos documentos fornecidos, n√£o encontrei uma resposta para isso.'
    3. N√£o utilize nenhum conhecimento externo.
    **Pergunta:** {question}
    **Resposta Especializada:**
    """
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm, 
        chain_type="stuff", 
        retriever=retriever, 
        return_source_documents=True, 
        chain_type_kwargs={"prompt": PROMPT}
    )
    logger.info("Cadeia RAG configurada com sucesso.")
    return qa_chain

# --- Interface Gr√°fica (Streamlit) ---

st.set_page_config(page_title="Assistente PESC/Coppe", page_icon="ü§ñ")
st.title("ü§ñ Assistente de Normas do PESC/Coppe")

# Mapeamento de nomes amig√°veis para nomes das cole√ß√µes
TOPICS = {
    "Bolsas acad√™micas": "bolsas_academicas",
    "Regras da Coppe": "regras_coppe_2020",
    "Roteiro para defesa": "roteiro_defesa_remota",
}

# Inicializa√ß√£o do estado da sess√£o
if "conversation_topic" not in st.session_state:
    st.session_state.conversation_topic = None
if "messages" not in st.session_state:
    st.session_state.messages = []
if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None

# --- L√≥gica da Aplica√ß√£o ---

# 1. Se nenhum t√≥pico foi escolhido, mostra a sele√ß√£o
if st.session_state.conversation_topic is None:
    st.info("Ol√°! Sou seu assistente para assuntos acad√™micos do PESC. Sobre qual t√≥pico voc√™ gostaria de conversar?")
    
    selected_topic_friendly_name = st.selectbox(
        "Escolha um assunto:",
        options=list(TOPICS.keys()),
        index=None,
        placeholder="Selecione um t√≥pico..."
    )

    if selected_topic_friendly_name:
        collection_name = TOPICS[selected_topic_friendly_name]
        st.session_state.conversation_topic = collection_name
        
        # Configura a cadeia RAG e a armazena na sess√£o
        with st.spinner(f"Preparando o assistente para falar sobre '{selected_topic_friendly_name}'..."):
            st.session_state.qa_chain = setup_qa_chain(collection_name)
        
        # Adiciona mensagem inicial do assistente
        st.session_state.messages.append({
            "role": "assistant",
            "content": f"Pronto! Pode me perguntar qualquer coisa sobre '{selected_topic_friendly_name}'.",
            "sources": [] # Mensagem inicial n√£o tem fontes
        })
        st.rerun()

# 2. Se um t√≥pico j√° foi escolhido, mostra a interface de chat
else:
    # Exibe as mensagens do hist√≥rico
    for i, message in enumerate(st.session_state.messages):
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            # Adiciona o bot√£o de fontes para respostas do assistente que tenham fontes
            if message["role"] == "assistant" and message["sources"]:
                with st.expander("Ver fontes da resposta"):
                    for doc in message["sources"]:
                        st.info(
                            f"**Fonte:** `{doc.metadata.get('source', 'N/A')}` | "
                            f"**P√°gina:** `{doc.metadata.get('page', 'N/A')}`\n\n"
                            f"**Conte√∫do:**\n\n---\n\n{doc.page_content}"
                        )
    
    # Captura a nova pergunta do usu√°rio
    if prompt := st.chat_input("Fa√ßa sua pergunta..."):
        # Adiciona a pergunta do usu√°rio ao hist√≥rico e exibe na tela
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Processa a pergunta e obt√©m a resposta do assistente
        with st.chat_message("assistant"):
            with st.spinner("Pensando..."):
                logger.info(f"Invocando a cadeia RAG com a query: '{prompt}'")
                response = st.session_state.qa_chain.invoke({"query": prompt})
                answer = response['result']
                source_documents = response['source_documents']
                
                st.markdown(answer)
                
                # Adiciona o bot√£o de fontes para a nova resposta
                with st.expander("Ver fontes da resposta"):
                    for doc in source_documents:
                        st.info(
                            f"**Fonte:** `{doc.metadata.get('source', 'N/A')}` | "
                            f"**P√°gina:** `{doc.metadata.get('page', 'N/A')}`\n\n"
                            f"**Conte√∫do:**\n\n---\n\n{doc.page_content}"
                        )
        
        # Adiciona a resposta completa do assistente (com fontes) ao hist√≥rico
        st.session_state.messages.append({
            "role": "assistant", 
            "content": answer, 
            "sources": source_documents
        })

